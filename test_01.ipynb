{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "sys.path.insert(1, \"source\")\n",
    "\n",
    "# dotenv_path = os.path.join(os.path.dirname(__file__), \".env\")\n",
    "# load_dotenv(dotenv_path)\n",
    "\n",
    "from prompts.agent_prompts import (\n",
    "    agent_manager_prompt,\n",
    "    data_agent_prompt,\n",
    "    model_agent_prompt,\n",
    "    prompt_agent,\n",
    "    operation_agent_prompt,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Groq client\n",
    "client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
    "api_key=os.environ.get(\"GROQ_API_KEY\")\n",
    "# print(api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "import logging\n",
    "import chromadb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from typing import Dict, Any\n",
    "from chromadb.config import Settings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import your existing classes with minor async modifications\n",
    "from source.state import State\n",
    "from source.memory import CSVEmbeddingManager  # Assuming you have these\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AsyncAgentBase:\n",
    "    \"\"\"Base class for all asynchronous agents\"\"\"\n",
    "    def __init__(self, role: str, model: str, description: str):\n",
    "        self.role = role\n",
    "        self.model = model\n",
    "        self.description = description\n",
    "        self.client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
    "\n",
    "    async def execute(self, messages: list) -> str:\n",
    "        \"\"\"Async version of execute method\"\"\"\n",
    "        try:\n",
    "            response = await asyncio.to_thread(\n",
    "                self.client.chat.completions.create,\n",
    "                messages=messages,\n",
    "                model=self.model\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in {self.role} agent: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "class AsyncAutoMLAgent(AsyncAgentBase):\n",
    "    \"\"\"Asynchronous AutoML agent with enhanced context handling\"\"\"\n",
    "    def __init__(self, *args, data_path: str = \"data\", **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.data_path = data_path\n",
    "        self.context = []\n",
    "\n",
    "    async def preprocess_data(self, instructions: str, context: list) -> str:\n",
    "        \"\"\"Enhanced preprocessing with context awareness\"\"\"\n",
    "        self.context.append((\"preprocessing\", instructions))\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": f\"Current context: {json.dumps(context)}\\n\\n{data_agent_prompt}\"},\n",
    "            {\"role\": \"user\", \"content\": instructions}\n",
    "        ]\n",
    "        return await self.execute(messages)\n",
    "\n",
    "class AsyncModelAgent(AsyncAgentBase):\n",
    "    \"\"\"Asynchronous Model agent with cross-agent awareness\"\"\"\n",
    "    async def retrieve_models(self, dataset_details: str, context: list) -> str:\n",
    "        \"\"\"Model selection with context from previous steps\"\"\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": f\"Pipeline Context: {json.dumps(context)}\\n\\n{model_agent_prompt}\"},\n",
    "            {\"role\": \"user\", \"content\": dataset_details}\n",
    "        ]\n",
    "        return await self.execute(messages)\n",
    "\n",
    "class AsyncOpsAgent(AsyncAgentBase):\n",
    "    \"\"\"Asynchronous Operations agent with deployment capabilities\"\"\"\n",
    "    async def deploy_model(self, deployment_details: str, context: list) -> str:\n",
    "        \"\"\"Context-aware deployment\"\"\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": f\"Full Context: {json.dumps(context)}\\n\\n{operation_agent_prompt}\"},\n",
    "            {\"role\": \"user\", \"content\": deployment_details}\n",
    "        ]\n",
    "        return await self.execute(messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import asyncio\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def split_text(text: str, max_chunk_length: int = 8000, overlap_ratio: float = 0.1):\n",
    "    \"\"\"\n",
    "    Splits a long string into overlapping chunks.\n",
    "    \"\"\"\n",
    "    if not (0 <= overlap_ratio < 1):\n",
    "        raise ValueError(\"Overlap ratio must be between 0 and 1 (exclusive).\")\n",
    "    \n",
    "    overlap_length = int(max_chunk_length * overlap_ratio)\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = min(start + max_chunk_length, len(text))\n",
    "        chunks.append(text[start:end])\n",
    "        start += max_chunk_length - overlap_length\n",
    "    return chunks\n",
    "\n",
    "class ImprovedCSVEmbeddingManager:\n",
    "    \"\"\"\n",
    "    ImprovedCSVEmbeddingManager embeds CSV data into a Chroma DB collection\n",
    "    using batch processing and optional text chunking.\n",
    "    \"\"\"\n",
    "    def __init__(self, collection_name=\"default_collection\", db_path=\"chromadb\", embedding_model=None, cache_size=10_000_000_000):\n",
    "        self.settings = Settings(\n",
    "            chroma_segment_cache_policy=\"LRU\",\n",
    "            chroma_memory_limit_bytes=cache_size\n",
    "        )\n",
    "        # Initialize persistent client for Chroma DB\n",
    "        self.client = chromadb.PersistentClient(path=db_path, settings=self.settings)\n",
    "        # Create or get the collection, specifying cosine similarity\n",
    "        self.collection = self.client.get_or_create_collection(collection_name, metadata={\"hnsw:space\": \"cosine\"})\n",
    "        if embedding_model is None:\n",
    "            raise ValueError(\"An embedding_model must be provided.\")\n",
    "        self.embedding_model = embedding_model\n",
    "        self.id_counter = 0  # To assign unique IDs if needed\n",
    "\n",
    "    def embed_csv(self, csv_file_path: str, batch_size: int = 1000):\n",
    "        \"\"\"\n",
    "        Reads a CSV file and embeds its content into the collection in batches.\n",
    "        Each row is converted to a JSON string (excluding the 'id' column if present).\n",
    "        If a row's text is too long, it is split into chunks.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(csv_file_path):\n",
    "            raise FileNotFoundError(f\"CSV file not found: {csv_file_path}\")\n",
    "        try:\n",
    "            df = pd.read_csv(csv_file_path)\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "        # Ensure there is an 'id' column; if not, create one\n",
    "        if 'id' not in df.columns:\n",
    "            df['id'] = df.index.astype(str)\n",
    "        \n",
    "        # Convert each row into a dictionary\n",
    "        rows = df.to_dict(orient='records')\n",
    "        \n",
    "        batch_ids = []\n",
    "        batch_documents = []\n",
    "        batch_metadatas = []\n",
    "        \n",
    "        for row in tqdm(rows, desc=\"Embedding CSV rows\"):\n",
    "            # Get the document id (as a string)\n",
    "            doc_id = str(row.get('id', self.id_counter))\n",
    "            # Remove the 'id' field for the embedding\n",
    "            row_copy = {k: v for k, v in row.items() if k != 'id'}\n",
    "            # Convert the remaining data to a JSON string\n",
    "            doc_text = json.dumps(row_copy)\n",
    "            \n",
    "            # Check if the document is too long; if so, split into chunks.\n",
    "            if len(doc_text) > 8000:\n",
    "                chunks = split_text(doc_text, max_chunk_length=8000, overlap_ratio=0.1)\n",
    "                for chunk in chunks:\n",
    "                    batch_documents.append(chunk)\n",
    "                    # Create a unique id for each chunk\n",
    "                    batch_ids.append(f\"{doc_id}_{self.id_counter}\")\n",
    "                    batch_metadatas.append({\"doc_name\": os.path.basename(csv_file_path)})\n",
    "                    self.id_counter += 1\n",
    "            else:\n",
    "                batch_documents.append(doc_text)\n",
    "                batch_ids.append(doc_id)\n",
    "                batch_metadatas.append({\"doc_name\": os.path.basename(csv_file_path)})\n",
    "                self.id_counter += 1\n",
    "\n",
    "            # If the batch is full, upsert into the collection in one call.\n",
    "            if len(batch_documents) >= batch_size:\n",
    "                embeddings = [self.embedding_model.encode(doc).tolist() for doc in batch_documents]\n",
    "                self.collection.add(\n",
    "                    documents=batch_documents,\n",
    "                    ids=batch_ids,\n",
    "                    embeddings=embeddings,\n",
    "                    metadatas=batch_metadatas\n",
    "                )\n",
    "                batch_ids = []\n",
    "                batch_documents = []\n",
    "                batch_metadatas = []\n",
    "\n",
    "        # Upsert any remaining documents not in a full batch.\n",
    "        if batch_documents:\n",
    "            embeddings = [self.embedding_model.encode(doc).tolist() for doc in batch_documents]\n",
    "            self.collection.add(\n",
    "                documents=batch_documents,\n",
    "                ids=batch_ids,\n",
    "                embeddings=embeddings,\n",
    "                metadatas=batch_metadatas\n",
    "            )\n",
    "        \n",
    "        print(f\"Finished embedding CSV: {csv_file_path}\")\n",
    "\n",
    "    import asyncio\n",
    "\n",
    "    async def query_collection(self, query: str, n_results: int = 5) -> dict:\n",
    "        \"\"\"Async version of query_collection.\"\"\"\n",
    "        query_embedding = await asyncio.to_thread(self.embedding_model.encode, query)\n",
    "        results = await asyncio.to_thread(self.collection.query, query_embeddings=query_embedding.tolist(), n_results=n_results, include=['documents', 'metadatas', 'distances'])\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class UnifiedPipeline:\n",
    "    \"\"\"Orchestrates the entire async pipeline with shared context and memory\"\"\"\n",
    "    def __init__(self, state: State, memory_manager: ImprovedCSVEmbeddingManager):\n",
    "        self.state = state\n",
    "        self.memory_manager = memory_manager\n",
    "        self.context_store = []\n",
    "        self.results = {}\n",
    "        \n",
    "        # Initialize agents\n",
    "        self.agents = {\n",
    "            \"automl\": AsyncAutoMLAgent(\n",
    "                role=\"data_scientist\",\n",
    "                model=\"llama-3.3-70b-versatile\",\n",
    "                description=\"AutoML expert\",\n",
    "                data_path=\"data\"\n",
    "            ),\n",
    "            \"model\": AsyncModelAgent(\n",
    "                role=\"ml_researcher\",\n",
    "                model=\"llama-3.3-70b-versatile\",\n",
    "                description=\"Model expert\"\n",
    "            ),\n",
    "            \"ops\": AsyncOpsAgent(\n",
    "                role=\"mlops\",\n",
    "                model=\"llama-3.3-70b-versatile\",\n",
    "                description=\"Deployment expert\"\n",
    "            )\n",
    "        }\n",
    "\n",
    "    async def _update_context(self, stage: str, output: str):\n",
    "        \"\"\"Update shared context with memory integration\"\"\"\n",
    "        # Store in memory\n",
    "        await self.memory_manager.embed_csv(output)\n",
    "        # Update pipeline context\n",
    "        self.context_store.append({stage: output})\n",
    "        self.state.update_memory({stage: output})\n",
    "        self.state.persist_memory()\n",
    "\n",
    "    async def run_pipeline(self, user_input: str) -> Dict[str, Any]:\n",
    "        \"\"\"Execute the full async pipeline with integrated context handling\"\"\"\n",
    "        try:\n",
    "            # Initial memory query\n",
    "            initial_context = await self.memory_manager.query_collection(user_input, n_results=3)\n",
    "            self.context_store = initial_context.get(\"documents\", [])\n",
    "            \n",
    "            # Parallel execution of pipeline stages\n",
    "            tasks = {\n",
    "                \"preprocessing\": self.agents[\"automl\"].preprocess_data(\n",
    "                    user_input, self.context_store\n",
    "                ),\n",
    "                \"model_selection\": self.agents[\"model\"].retrieve_models(\n",
    "                    \"Find top models\", self.context_store\n",
    "                ),\n",
    "                \"deployment\": self.agents[\"ops\"].deploy_model(\n",
    "                    \"Deploy best model\", self.context_store\n",
    "                )\n",
    "            }\n",
    "            \n",
    "            # Run all tasks concurrently\n",
    "            completed = await asyncio.gather(*tasks.values(), return_exceptions=True)\n",
    "            \n",
    "            # Process results\n",
    "            for stage, result in zip(tasks.keys(), completed):\n",
    "                if isinstance(result, Exception):\n",
    "                    logger.error(f\"Error in {stage}: {str(result)}\")\n",
    "                    continue\n",
    "                \n",
    "                await self._update_context(stage, result)\n",
    "                self.results[stage] = result\n",
    "\n",
    "            # Generate unified output\n",
    "            return await self._generate_unified_output()\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Pipeline failed: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    async def _generate_unified_output(self) -> str:\n",
    "        \"\"\"Create a single coherent output from all pipeline stages\"\"\"\n",
    "        unified = [\n",
    "            \"# Automated ML Pipeline Report\",\n",
    "            \"## Context Summary\",\n",
    "            f\"Pipeline Phase: {self.state.phase}\",\n",
    "            f\"Competition: {self.state.competition}\",\n",
    "            \"### Memory Context:\",\n",
    "            *[f\"- {ctx}\" for ctx in self.context_store[-3:]],\n",
    "            \"\\n## Pipeline Execution Details\"\n",
    "        ]\n",
    "        \n",
    "        for stage, result in self.results.items():\n",
    "            unified.extend([\n",
    "                f\"\\n### {stage.replace('_', ' ').title()}\",\n",
    "                f\"```\\n{result}\\n```\"\n",
    "            ])\n",
    "            \n",
    "        unified.append(\"\\n## Final Recommendations\")\n",
    "        unified.append(await self._generate_summary())\n",
    "        \n",
    "        return \"\\n\".join(unified)\n",
    "\n",
    "    async def _generate_summary(self) -> str:\n",
    "        \"\"\"Generate final summary using context\"\"\"\n",
    "        summary_prompt = f\"\"\"\n",
    "        Generate a comprehensive summary of the ML pipeline execution using this context:\n",
    "        {json.dumps(self.context_store)}\n",
    "        \"\"\"\n",
    "        return await self.agents[\"automl\"].execute([\n",
    "            {\"role\": \"system\", \"content\": \"Summarize the pipeline execution\"},\n",
    "            {\"role\": \"user\", \"content\": summary_prompt}\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: mps\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 42.55it/s]\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "ERROR:__main__:Pipeline failed: CSV file not found: **Rent the Runway Dataset Analysis and Model Deployment**\n",
      "\n",
      "### Dataset Overview\n",
      "\n",
      "The provided dataset contains information about women's clothing fit, including user ID, item ID, category, size, body type, bust size, height, weight, age, and rating. The dataset is used to predict the fit of clothing items for users.\n",
      "\n",
      "### Data Preprocessing\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "\n",
      "# Load the dataset\n",
      "data = pd.read_json('[{\"fit\": \"fit\", \"user_id\": 978643, \"bust size\": \"34a\", \"item_id\": 144714, \"weight\": NaN, \"rating\": 10.0, \"body type\": \"athletic\", \"category\": \"gown\", \"height\": 170.18, \"size\": 8, \"age\": 26.0}, \\\n",
      "                    {\"fit\": \"fit\", \"user_id\": 510619, \"bust size\": \"34dd\", \"item_id\": 1833819, \"weight\": NaN, \"rating\": 10.0, \"body type\": \"athletic\", \"category\": \"gown\", \"height\": 157.48000000000002, \"size\": 16, \"age\": 34.0}, \\\n",
      "                    {\"fit\": \"fit\", \"user_id\": 316065, \"bust size\": \"32d\", \"item_id\": 1585757, \"weight\": 53.523856, \"rating\": 10.0, \"body type\": NaN, \"category\": \"gown\", \"height\": 157.48000000000002, \"size\": 4, \"age\": 38.0}]')\n",
      "\n",
      "# Drop rows with missing values\n",
      "data.dropna(inplace=True)\n",
      "\n",
      "# Encode categorical variables\n",
      "le = LabelEncoder()\n",
      "data['category'] = le.fit_transform(data['category'])\n",
      "data['body type'] = le.fit_transform(data['body type'].fillna('Unknown'))\n",
      "data['fit'] = le.fit_transform(data['fit'])\n",
      "\n",
      "# Split data into training and testing sets\n",
      "X = data.drop(['fit'], axis=1)\n",
      "y = data['fit']\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "```\n",
      "\n",
      "### Data Augmentation\n",
      "\n",
      "To improve the model's performance, we can create additional features through data augmentation. One possible approach is to extract features from the user's body type and clothing size.\n",
      "\n",
      "```python\n",
      "# Create new features\n",
      "data['body_type_size'] = data['body type'] * data['size']\n",
      "data['height_weight_ratio'] = data['height'] / data['weight']\n",
      "```\n",
      "\n",
      "### Model Selection and Training\n",
      "\n",
      "To achieve an F1 score of ≥90%, we can use a random forest classifier with hyperparameter tuning.\n",
      "\n",
      "```python\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "\n",
      "# Define hyperparameter tuning space\n",
      "param_grid = {\n",
      "    'n_estimators': [100, 200, 300],\n",
      "    'max_depth': [None, 5, 10],\n",
      "    'min_samples_split': [2, 5, 10],\n",
      "    'min_samples_leaf': [1, 5, 10]\n",
      "}\n",
      "\n",
      "# Perform hyperparameter tuning\n",
      "rf = RandomForestClassifier()\n",
      "grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='f1_macro')\n",
      "grid_search.fit(X_train, y_train)\n",
      "\n",
      "# Train the model with the best hyperparameters\n",
      "best_model = grid_search.best_estimator_\n",
      "best_model.fit(X_train, y_train)\n",
      "```\n",
      "\n",
      "### Model Evaluation\n",
      "\n",
      "```python\n",
      "from sklearn.metrics import f1_score\n",
      "\n",
      "# Evaluate the model on the test set\n",
      "y_pred = best_model.predict(X_test)\n",
      "f1 = f1_score(y_test, y_pred, average='macro')\n",
      "print(f'F1 score: {f1:.3f}')\n",
      "```\n",
      "\n",
      "### Model Deployment\n",
      "\n",
      "To deploy the model as a web service, we can use Flask and create a RESTful API.\n",
      "\n",
      "```python\n",
      "from flask import Flask, request, jsonify\n",
      "import pickle\n",
      "\n",
      "app = Flask(__name__)\n",
      "\n",
      "# Load the trained model\n",
      "with open('best_model.pkl', 'rb') as f:\n",
      "    best_model = pickle.load(f)\n",
      "\n",
      "# Define the API endpoint\n",
      "@app.route('/predict', methods=['POST'])\n",
      "def predict():\n",
      "    data = request.get_json()\n",
      "    # Preprocess the input data\n",
      "    input_data = pd.DataFrame([data])\n",
      "    input_data = input_data.drop(['fit'], axis=1)\n",
      "    input_data['category'] = le.transform(input_data['category'])\n",
      "    input_data['body type'] = le.transform(input_data['body type'].fillna('Unknown'))\n",
      "    # Make predictions\n",
      "    predictions = best_model.predict(input_data)\n",
      "    return jsonify({'prediction': predictions.tolist()})\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    app.run(debug=True)\n",
      "```\n",
      "\n",
      "**Example Use Case**\n",
      "\n",
      "To use the deployed model, send a POST request to the `/predict` endpoint with the input data in JSON format.\n",
      "\n",
      "```bash\n",
      "curl -X POST -H \"Content-Type: application/json\" -d '{\"user_id\": 978643, \"item_id\": 144714, \"category\": \"gown\", \"size\": 8, \"body type\": \"athletic\", \"height\": 170.18, \"weight\": 53.523856, \"age\": 26.0}' http://localhost:5000/predict\n",
      "```\n",
      "\n",
      "This should return a JSON response with the predicted fit.\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"prediction\": [1]\n",
      "}\n",
      "```\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "CSV file not found: **Rent the Runway Dataset Analysis and Model Deployment**\n\n### Dataset Overview\n\nThe provided dataset contains information about women's clothing fit, including user ID, item ID, category, size, body type, bust size, height, weight, age, and rating. The dataset is used to predict the fit of clothing items for users.\n\n### Data Preprocessing\n\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n# Load the dataset\ndata = pd.read_json('[{\"fit\": \"fit\", \"user_id\": 978643, \"bust size\": \"34a\", \"item_id\": 144714, \"weight\": NaN, \"rating\": 10.0, \"body type\": \"athletic\", \"category\": \"gown\", \"height\": 170.18, \"size\": 8, \"age\": 26.0}, \\\n                    {\"fit\": \"fit\", \"user_id\": 510619, \"bust size\": \"34dd\", \"item_id\": 1833819, \"weight\": NaN, \"rating\": 10.0, \"body type\": \"athletic\", \"category\": \"gown\", \"height\": 157.48000000000002, \"size\": 16, \"age\": 34.0}, \\\n                    {\"fit\": \"fit\", \"user_id\": 316065, \"bust size\": \"32d\", \"item_id\": 1585757, \"weight\": 53.523856, \"rating\": 10.0, \"body type\": NaN, \"category\": \"gown\", \"height\": 157.48000000000002, \"size\": 4, \"age\": 38.0}]')\n\n# Drop rows with missing values\ndata.dropna(inplace=True)\n\n# Encode categorical variables\nle = LabelEncoder()\ndata['category'] = le.fit_transform(data['category'])\ndata['body type'] = le.fit_transform(data['body type'].fillna('Unknown'))\ndata['fit'] = le.fit_transform(data['fit'])\n\n# Split data into training and testing sets\nX = data.drop(['fit'], axis=1)\ny = data['fit']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n```\n\n### Data Augmentation\n\nTo improve the model's performance, we can create additional features through data augmentation. One possible approach is to extract features from the user's body type and clothing size.\n\n```python\n# Create new features\ndata['body_type_size'] = data['body type'] * data['size']\ndata['height_weight_ratio'] = data['height'] / data['weight']\n```\n\n### Model Selection and Training\n\nTo achieve an F1 score of ≥90%, we can use a random forest classifier with hyperparameter tuning.\n\n```python\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\n# Define hyperparameter tuning space\nparam_grid = {\n    'n_estimators': [100, 200, 300],\n    'max_depth': [None, 5, 10],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 5, 10]\n}\n\n# Perform hyperparameter tuning\nrf = RandomForestClassifier()\ngrid_search = GridSearchCV(rf, param_grid, cv=5, scoring='f1_macro')\ngrid_search.fit(X_train, y_train)\n\n# Train the model with the best hyperparameters\nbest_model = grid_search.best_estimator_\nbest_model.fit(X_train, y_train)\n```\n\n### Model Evaluation\n\n```python\nfrom sklearn.metrics import f1_score\n\n# Evaluate the model on the test set\ny_pred = best_model.predict(X_test)\nf1 = f1_score(y_test, y_pred, average='macro')\nprint(f'F1 score: {f1:.3f}')\n```\n\n### Model Deployment\n\nTo deploy the model as a web service, we can use Flask and create a RESTful API.\n\n```python\nfrom flask import Flask, request, jsonify\nimport pickle\n\napp = Flask(__name__)\n\n# Load the trained model\nwith open('best_model.pkl', 'rb') as f:\n    best_model = pickle.load(f)\n\n# Define the API endpoint\n@app.route('/predict', methods=['POST'])\ndef predict():\n    data = request.get_json()\n    # Preprocess the input data\n    input_data = pd.DataFrame([data])\n    input_data = input_data.drop(['fit'], axis=1)\n    input_data['category'] = le.transform(input_data['category'])\n    input_data['body type'] = le.transform(input_data['body type'].fillna('Unknown'))\n    # Make predictions\n    predictions = best_model.predict(input_data)\n    return jsonify({'prediction': predictions.tolist()})\n\nif __name__ == '__main__':\n    app.run(debug=True)\n```\n\n**Example Use Case**\n\nTo use the deployed model, send a POST request to the `/predict` endpoint with the input data in JSON format.\n\n```bash\ncurl -X POST -H \"Content-Type: application/json\" -d '{\"user_id\": 978643, \"item_id\": 144714, \"category\": \"gown\", \"size\": 8, \"body type\": \"athletic\", \"height\": 170.18, \"weight\": 53.523856, \"age\": 26.0}' http://localhost:5000/predict\n```\n\nThis should return a JSON response with the predicted fit.\n\n```json\n{\n    \"prediction\": [1]\n}\n```",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 65\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnest_asyncio\u001b[39;00m\n\u001b[1;32m     64\u001b[0m nest_asyncio\u001b[38;5;241m.\u001b[39mapply()\n\u001b[0;32m---> 65\u001b[0m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/multi-agent/lib/python3.10/site-packages/nest_asyncio.py:30\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     28\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[0;32m~/miniconda3/envs/multi-agent/lib/python3.10/site-packages/nest_asyncio.py:98\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/multi-agent/lib/python3.10/asyncio/futures.py:201\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[0;32m~/miniconda3/envs/multi-agent/lib/python3.10/asyncio/tasks.py:232\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    230\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 232\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    234\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "Cell \u001b[0;32mIn[38], line 48\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m user_input \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI have uploaded the Rent the Runway dataset for women\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms clothing fit prediction. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDevelop a model with ≥90\u001b[39m\u001b[38;5;132;01m% F\u001b[39;00m\u001b[38;5;124m1 score and deploy as a web service.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     45\u001b[0m )\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Execute pipeline.\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m final_report \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m pipeline\u001b[38;5;241m.\u001b[39mrun_pipeline(user_input)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Save the final report as a Markdown file.\u001b[39;00m\n\u001b[1;32m     51\u001b[0m final_md_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_report.md\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[37], line 67\u001b[0m, in \u001b[0;36mUnifiedPipeline.run_pipeline\u001b[0;34m(self, user_input)\u001b[0m\n\u001b[1;32m     64\u001b[0m         logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(result)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     65\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_context(stage, result)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults[stage] \u001b[38;5;241m=\u001b[39m result\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Generate unified output\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[37], line 32\u001b[0m, in \u001b[0;36mUnifiedPipeline._update_context\u001b[0;34m(self, stage, output)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Update shared context with memory integration\"\"\"\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Store in memory\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemory_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Update pipeline context\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_store\u001b[38;5;241m.\u001b[39mappend({stage: output})\n",
      "Cell \u001b[0;32mIn[36], line 53\u001b[0m, in \u001b[0;36mImprovedCSVEmbeddingManager.embed_csv\u001b[0;34m(self, csv_file_path, batch_size)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;124;03mReads a CSV file and embeds its content into the collection in batches.\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03mEach row is converted to a JSON string (excluding the 'id' column if present).\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03mIf a row's text is too long, it is split into chunks.\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(csv_file_path):\n\u001b[0;32m---> 53\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCSV file not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcsv_file_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     55\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(csv_file_path)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: CSV file not found: **Rent the Runway Dataset Analysis and Model Deployment**\n\n### Dataset Overview\n\nThe provided dataset contains information about women's clothing fit, including user ID, item ID, category, size, body type, bust size, height, weight, age, and rating. The dataset is used to predict the fit of clothing items for users.\n\n### Data Preprocessing\n\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n# Load the dataset\ndata = pd.read_json('[{\"fit\": \"fit\", \"user_id\": 978643, \"bust size\": \"34a\", \"item_id\": 144714, \"weight\": NaN, \"rating\": 10.0, \"body type\": \"athletic\", \"category\": \"gown\", \"height\": 170.18, \"size\": 8, \"age\": 26.0}, \\\n                    {\"fit\": \"fit\", \"user_id\": 510619, \"bust size\": \"34dd\", \"item_id\": 1833819, \"weight\": NaN, \"rating\": 10.0, \"body type\": \"athletic\", \"category\": \"gown\", \"height\": 157.48000000000002, \"size\": 16, \"age\": 34.0}, \\\n                    {\"fit\": \"fit\", \"user_id\": 316065, \"bust size\": \"32d\", \"item_id\": 1585757, \"weight\": 53.523856, \"rating\": 10.0, \"body type\": NaN, \"category\": \"gown\", \"height\": 157.48000000000002, \"size\": 4, \"age\": 38.0}]')\n\n# Drop rows with missing values\ndata.dropna(inplace=True)\n\n# Encode categorical variables\nle = LabelEncoder()\ndata['category'] = le.fit_transform(data['category'])\ndata['body type'] = le.fit_transform(data['body type'].fillna('Unknown'))\ndata['fit'] = le.fit_transform(data['fit'])\n\n# Split data into training and testing sets\nX = data.drop(['fit'], axis=1)\ny = data['fit']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n```\n\n### Data Augmentation\n\nTo improve the model's performance, we can create additional features through data augmentation. One possible approach is to extract features from the user's body type and clothing size.\n\n```python\n# Create new features\ndata['body_type_size'] = data['body type'] * data['size']\ndata['height_weight_ratio'] = data['height'] / data['weight']\n```\n\n### Model Selection and Training\n\nTo achieve an F1 score of ≥90%, we can use a random forest classifier with hyperparameter tuning.\n\n```python\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\n# Define hyperparameter tuning space\nparam_grid = {\n    'n_estimators': [100, 200, 300],\n    'max_depth': [None, 5, 10],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 5, 10]\n}\n\n# Perform hyperparameter tuning\nrf = RandomForestClassifier()\ngrid_search = GridSearchCV(rf, param_grid, cv=5, scoring='f1_macro')\ngrid_search.fit(X_train, y_train)\n\n# Train the model with the best hyperparameters\nbest_model = grid_search.best_estimator_\nbest_model.fit(X_train, y_train)\n```\n\n### Model Evaluation\n\n```python\nfrom sklearn.metrics import f1_score\n\n# Evaluate the model on the test set\ny_pred = best_model.predict(X_test)\nf1 = f1_score(y_test, y_pred, average='macro')\nprint(f'F1 score: {f1:.3f}')\n```\n\n### Model Deployment\n\nTo deploy the model as a web service, we can use Flask and create a RESTful API.\n\n```python\nfrom flask import Flask, request, jsonify\nimport pickle\n\napp = Flask(__name__)\n\n# Load the trained model\nwith open('best_model.pkl', 'rb') as f:\n    best_model = pickle.load(f)\n\n# Define the API endpoint\n@app.route('/predict', methods=['POST'])\ndef predict():\n    data = request.get_json()\n    # Preprocess the input data\n    input_data = pd.DataFrame([data])\n    input_data = input_data.drop(['fit'], axis=1)\n    input_data['category'] = le.transform(input_data['category'])\n    input_data['body type'] = le.transform(input_data['body type'].fillna('Unknown'))\n    # Make predictions\n    predictions = best_model.predict(input_data)\n    return jsonify({'prediction': predictions.tolist()})\n\nif __name__ == '__main__':\n    app.run(debug=True)\n```\n\n**Example Use Case**\n\nTo use the deployed model, send a POST request to the `/predict` endpoint with the input data in JSON format.\n\n```bash\ncurl -X POST -H \"Content-Type: application/json\" -d '{\"user_id\": 978643, \"item_id\": 144714, \"category\": \"gown\", \"size\": 8, \"body type\": \"athletic\", \"height\": 170.18, \"weight\": 53.523856, \"age\": 26.0}' http://localhost:5000/predict\n```\n\nThis should return a JSON response with the predicted fit.\n\n```json\n{\n    \"prediction\": [1]\n}\n```"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Usage Example and Main Execution\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "async def main():\n",
    "    # Initialize state.\n",
    "    state = State(phase=\"Model Development\", competition=\"MyCompetition\")\n",
    "    \n",
    "    # Initialize the embedding model.\n",
    "    embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    \n",
    "    # Instantiate the CSV embedding manager.\n",
    "    memory_manager = ImprovedCSVEmbeddingManager(\n",
    "        collection_name=\"auto_ml_memory\",\n",
    "        embedding_model=embedding_model\n",
    "    )\n",
    "    \n",
    "    # Ensure the data directory exists.\n",
    "    data_dir = \"data\"\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "    \n",
    "    csv_file_path = os.path.join(data_dir, \"renttherunway_cleaned.csv\")\n",
    "    \n",
    "    # For demonstration, if the CSV doesn't exist, create a dummy CSV.\n",
    "    if not os.path.exists(csv_file_path):\n",
    "        df_dummy = pd.DataFrame({\n",
    "            \"id\": [1, 2],\n",
    "            \"feature\": [\"value1\", \"value2\"],\n",
    "            \"label\": [\"A\", \"B\"]\n",
    "        })\n",
    "        df_dummy.to_csv(csv_file_path, index=False)\n",
    "    \n",
    "    # Embed dataset if not already embedded.\n",
    "    if memory_manager.collection.count() == 0:\n",
    "        memory_manager.embed_csv(csv_file_path)\n",
    "    \n",
    "    # Create the pipeline.\n",
    "    pipeline = UnifiedPipeline(state, memory_manager)\n",
    "    \n",
    "    # Define user input.\n",
    "    user_input = (\n",
    "        \"I have uploaded the Rent the Runway dataset for women's clothing fit prediction. \"\n",
    "        \"Develop a model with ≥90% F1 score and deploy as a web service.\"\n",
    "    )\n",
    "    \n",
    "    # Execute pipeline.\n",
    "    final_report = await pipeline.run_pipeline(user_input)\n",
    "    \n",
    "    # Save the final report as a Markdown file.\n",
    "    final_md_path = os.path.join(data_dir, \"final_report.md\")\n",
    "    with open(final_md_path, \"w\") as f:\n",
    "        f.write(\"## Final Pipeline Report\\n\")\n",
    "        f.write(final_report)\n",
    "        f.write(\"\\n\")\n",
    "    \n",
    "    print(\"Pipeline execution completed. Final report saved at:\", final_md_path)\n",
    "    print(\"Final Report:\")\n",
    "    print(final_report)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Allow nested event loops if needed.\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multi-agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
