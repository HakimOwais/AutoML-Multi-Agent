{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "sys.path.insert(1, \"source\")\n",
    "\n",
    "# dotenv_path = os.path.join(os.path.dirname(__file__), \".env\")\n",
    "# load_dotenv(dotenv_path)\n",
    "\n",
    "from prompts.agent_prompts import (\n",
    "    agent_manager_prompt,\n",
    "    data_agent_prompt,\n",
    "    model_agent_prompt,\n",
    "    prompt_agent,\n",
    "    operation_agent_prompt,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Groq client\n",
    "client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
    "api_key=os.environ.get(\"GROQ_API_KEY\")\n",
    "# print(api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# 1. AGENT BASE CLASSES #\n",
    "#########################\n",
    "\n",
    "class AgentBase:\n",
    "    def __init__(self, role, model, description, **kwargs):\n",
    "        self.role = role\n",
    "        self.model = model\n",
    "        self.description = description\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    def execute(self, messages):\n",
    "        \"\"\"Executes a task using the defined role and model.\"\"\"\n",
    "        return client.chat.completions.create(\n",
    "            messages=messages,\n",
    "            model=self.model,\n",
    "            **self.kwargs\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Manager Agent (inherits from AgentBase)\n",
    "# ----------------------------\n",
    "class AgentManager(AgentBase):\n",
    "    def __init__(self, role, model, description, json_schema, **kwargs):\n",
    "        super().__init__(role, model, description, **kwargs)\n",
    "        self.json_schema = json_schema\n",
    "\n",
    "    def parse_to_json(self, user_input):\n",
    "        \"\"\"Parses the user input into a JSON format based on the schema.\"\"\"\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"\"\"\n",
    "{agent_manager_prompt.strip()}\n",
    "\n",
    "# JSON SPECIFICATION SCHEMA #\n",
    "{self.json_schema}\n",
    "\"\"\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": user_input},\n",
    "        ]\n",
    "        response = self.execute(messages)\n",
    "        return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------\n",
    "# Prompt Agent (inherits from AgentBase)\n",
    "# ----------------------------\n",
    "class PromptAgent(AgentBase):\n",
    "    def __init__(self, role, model, description, json_specification, **kwargs):\n",
    "        super().__init__(role, model, description, **kwargs)\n",
    "        self.json_specification = json_specification\n",
    "\n",
    "    def generate_json(self, user_input):\n",
    "        \"\"\"Generates a JSON response strictly adhering to the specification.\"\"\"\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"\"\"\n",
    "{prompt_agent.strip()}\n",
    "\n",
    "# JSON SPECIFICATION SCHEMA #\n",
    "'''json\n",
    "{self.json_specification}\n",
    "'''\n",
    "\"\"\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": user_input},\n",
    "        ]\n",
    "        response = self.execute(messages)\n",
    "        return response.choices[0].message.content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# AutoML Agent (inherits from AgentBase)\n",
    "# ----------------------------\n",
    "class AutoMLAgent(AgentBase):\n",
    "    def __init__(self, role, model, description, data_path=\"./data\", **kwargs):\n",
    "        super().__init__(role, model, description, **kwargs)\n",
    "        self.data_path = data_path\n",
    "\n",
    "    def retrieve_dataset(self, query):\n",
    "        \"\"\"Retrieves a dataset based on user instructions or searches for one.\"\"\"\n",
    "        dataset_path = os.path.join(self.data_path, \"renttherunway_cleaned.csv\")\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": data_agent_prompt.strip()},\n",
    "            {\"role\": \"user\", \"content\": query},\n",
    "        ]\n",
    "        response = self.execute(messages)\n",
    "        # Save the retrieved dataset to the specified path (placeholder implementation)\n",
    "        with open(dataset_path, \"w\") as file:\n",
    "            file.write(response.choices[0].message.content)\n",
    "        return dataset_path\n",
    "\n",
    "    def preprocess_data(self, instructions):\n",
    "        \"\"\"Performs data preprocessing based on user instructions or best practices.\"\"\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": data_agent_prompt.strip()},\n",
    "            {\"role\": \"user\", \"content\": f\"Instructions: {instructions}\"},\n",
    "        ]\n",
    "        response = self.execute(messages)\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    def augment_data(self, augmentation_details):\n",
    "        \"\"\"Performs data augmentation as necessary.\"\"\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": data_agent_prompt.strip()},\n",
    "            {\"role\": \"user\", \"content\": f\"Augmentation Details: {augmentation_details}\"},\n",
    "        ]\n",
    "        response = self.execute(messages)\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    def visualize_data(self, visualization_request):\n",
    "        \"\"\"Generates meaningful visualizations to understand the dataset.\"\"\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": data_agent_prompt.strip()},\n",
    "            {\"role\": \"user\", \"content\": visualization_request},\n",
    "        ]\n",
    "        response = self.execute(messages)\n",
    "        return response.choices[0].message.content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------\n",
    "# Model Agent (inherits from AgentBase)\n",
    "# ----------------------------\n",
    "class ModelAgent(AgentBase):\n",
    "    def __init__(self, role, model, description, **kwargs):\n",
    "        super().__init__(role, model, description, **kwargs)\n",
    "\n",
    "    def retrieve_models(self, dataset_details):\n",
    "        \"\"\"Retrieve a list of well-performing models or algorithms based on dataset details.\"\"\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": model_agent_prompt.strip()},\n",
    "            {\"role\": \"user\", \"content\": dataset_details},\n",
    "        ]\n",
    "        response = self.execute(messages)\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    def optimize_model(self, hyperparameter_details):\n",
    "        \"\"\"Perform hyperparameter optimization on candidate models.\"\"\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": model_agent_prompt.strip()},\n",
    "            {\"role\": \"user\", \"content\": hyperparameter_details},\n",
    "        ]\n",
    "        response = self.execute(messages)\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    def profile_models(self, profiling_details):\n",
    "        \"\"\"Perform metadata extraction and profiling on candidate models.\"\"\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": model_agent_prompt.strip()},\n",
    "            {\"role\": \"user\", \"content\": profiling_details},\n",
    "        ]\n",
    "        response = self.execute(messages)\n",
    "        return response.choices[0].message.content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Operations Agent (inherits from AgentBase)\n",
    "# ----------------------------\n",
    "class OperationsAgent(AgentBase):\n",
    "    def __init__(self, role, model, description, **kwargs):\n",
    "        super().__init__(role, model, description, **kwargs)\n",
    "\n",
    "    def deploy_model(self, deployment_details):\n",
    "        \"\"\"Prepare and deploy the model based on the provided details.\"\"\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": operation_agent_prompt.strip()},\n",
    "            {\"role\": \"user\", \"content\": deployment_details},\n",
    "        ]\n",
    "        response = self.execute(messages)\n",
    "        return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "# 2. AGENT INSTANTIATION SETUP  #\n",
    "#################################\n",
    "\n",
    "# Define JSON specification schema\n",
    "JSON_SCHEMA = \"\"\"json\n",
    "{\n",
    "    \"task\": \"string\",\n",
    "    \"priority\": \"string\",\n",
    "    \"deadline\": \"string\",\n",
    "    \"resources\": [\n",
    "        {\n",
    "            \"type\": \"string\",\n",
    "            \"quantity\": \"integer\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Create agent instances\n",
    "manager_agent = AgentManager(\n",
    "    role=\"manager\",\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    description=\"Assistant project manager for parsing user requirements into JSON.\",\n",
    "    json_schema=JSON_SCHEMA,\n",
    "    stream=False\n",
    ")\n",
    "\n",
    "prompt_parser_agent = PromptAgent(\n",
    "    role=\"prompt_parser\",\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    description=\"Assistant project manager for JSON parsing.\",\n",
    "    json_specification=JSON_SCHEMA,\n",
    "    stream=False\n",
    ")\n",
    "\n",
    "automl_agent = AutoMLAgent(\n",
    "    role=\"data_scientist\",\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    description=\"Automated machine learning agent for dataset retrieval, preprocessing, augmentation, and visualization.\",\n",
    "    data_path=\"data\",\n",
    "    stream=False\n",
    ")\n",
    "\n",
    "model_agent = ModelAgent(\n",
    "    role=\"ml_researcher\",\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    description=\"Machine learning research agent for model optimization and profiling.\",\n",
    "    stream=False\n",
    ")\n",
    "\n",
    "operations_agent = OperationsAgent(\n",
    "    role=\"mlops\",\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    description=\"MLOps agent for deployment and application development.\",\n",
    "    stream=False\n",
    ")\n",
    "\n",
    "# A dictionary to hold all agents if needed\n",
    "agents = {\n",
    "    \"manager\": manager_agent,\n",
    "    \"prompt\": prompt_parser_agent,\n",
    "    \"automl\": automl_agent,\n",
    "    \"model\": model_agent,\n",
    "    \"operations\": operations_agent\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.state import State\n",
    "from source.memory import CSVEmbeddingManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#################################\n",
    "# 3. PIPELINE AGENT DEFINITION  #\n",
    "#################################\n",
    "\n",
    "class PipelineAgent(AgentBase):\n",
    "    \"\"\"\n",
    "    PipelineAgent is responsible for orchestrating the full data-to-deployment pipeline.\n",
    "    It uses the various agents (data, model, ops, etc.) to execute their tasks sequentially.\n",
    "    \"\"\"\n",
    "    def __init__(self, agents, state: State,memory_manager: CSVEmbeddingManager,\n",
    "                 dataset_dir=\"data\", **kwargs):\n",
    "        # You can give this pipeline a role and model description if needed.\n",
    "        super().__init__(role=\"pipeline\", model=\"n/a\", description=\"Pipeline to orchestrate all agents\", **kwargs)\n",
    "        self.agents = agents\n",
    "        self.state = state\n",
    "        self.memory_manager = memory_manager\n",
    "        self.dataset_dir = dataset_dir\n",
    "\n",
    "    def run_pipeline(self, preprocessing_input, model_request, deployment_details):\n",
    "        \"\"\"\n",
    "        Executes a full pipeline:\n",
    "          1. Preprocess data.\n",
    "          2. Retrieve candidate models.\n",
    "          3. Deploy the selected model.\n",
    "        \"\"\"\n",
    "        os.makedirs(self.dataset_dir, exist_ok=True)\n",
    "        self.state.make_dir()\n",
    "\n",
    "        # 1. Preprocess the dataset using the AutoML agent\n",
    "        preprocessed_data = self.agents[\"automl\"].preprocess_data(preprocessing_input)\n",
    "        preprocessed_path = os.path.join(self.dataset_dir, \"preprocessed_data.md\")\n",
    "        with open(preprocessed_path, \"w\") as f:\n",
    "            f.write(preprocessed_data)\n",
    "        print(f\"Preprocessed data saved to: {preprocessed_path}\")\n",
    "\n",
    "        # Update state memory for preprocessing step\n",
    "        self.state.update_memory({\"preprocessing\": preprocessed_data})\n",
    "        self.state.persist_memory() # save memory to disk\n",
    "        # Optionally update csv embedding if preprocessing produces a CSV\n",
    "        # self.memory_manager.update_embedding(preprocesses_csv_file)\n",
    "\n",
    "        # Advance state and write current agents rules\n",
    "        rules = self.state.generate_rules()\n",
    "        print(\"[Pipeline] Current agent rules saved:\\n\", rules)\n",
    "        self.state.next_step()\n",
    "\n",
    "        # 2. Retrieve candidate models using the Model agent\n",
    "        model_list = self.agents[\"model\"].retrieve_models(model_request)\n",
    "        model_list_path = os.path.join(self.dataset_dir, \"model_list.md\")\n",
    "        with open(model_list_path, \"w\") as f:\n",
    "            f.write(model_list)\n",
    "        print(f\"Model list saved to: {model_list_path}\")\n",
    "\n",
    "        # Update state memory for model retrieval step\n",
    "        self.state.update_memory({\"model_list\": model_list})\n",
    "        self.state.persist_memory()\n",
    "        self.state.next_step()\n",
    "\n",
    "        # 3. Deploy the model using the Operations agent\n",
    "        deployment_output = self.agents[\"operations\"].deploy_model(deployment_details)\n",
    "        deployment_output_path = os.path.join(self.dataset_dir, \"deployment_output.md\")\n",
    "        with open(deployment_output_path, \"w\") as f:\n",
    "            f.write(deployment_output)\n",
    "        print(f\"Deployment output saved to: {deployment_output_path}\")\n",
    "\n",
    "        # Update state memory for deployment step\n",
    "        self.state.update_memory({\"deployment_output\": deployment_output})\n",
    "        self.state.persist_memory()\n",
    "        self.state.next_step()\n",
    "\n",
    "        # Return a dictionary of results (if needed)\n",
    "        return {\n",
    "            \"preprocessed_data\": preprocessed_data,\n",
    "            \"model_list\": model_list,\n",
    "            \"deployment_output\": deployment_output\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "def split_text(text: str, max_chunk_length: int = 8000, overlap_ratio: float = 0.1):\n",
    "    \"\"\"\n",
    "    Splits a long string into overlapping chunks.\n",
    "    \"\"\"\n",
    "    if not (0 <= overlap_ratio < 1):\n",
    "        raise ValueError(\"Overlap ratio must be between 0 and 1 (exclusive).\")\n",
    "    \n",
    "    overlap_length = int(max_chunk_length * overlap_ratio)\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = min(start + max_chunk_length, len(text))\n",
    "        chunks.append(text[start:end])\n",
    "        start += max_chunk_length - overlap_length\n",
    "    return chunks\n",
    "\n",
    "class ImprovedCSVEmbeddingManager:\n",
    "    \"\"\"\n",
    "    ImprovedCSVEmbeddingManager embeds CSV data into a Chroma DB collection\n",
    "    using batch processing and optional text chunking. This should speed up the\n",
    "    embedding process compared to row-by-row insertion.\n",
    "    \"\"\"\n",
    "    def __init__(self, collection_name=\"default_collection\", db_path=\"chromadb\", embedding_model=None, cache_size=10_000_000_000):\n",
    "        self.settings = Settings(\n",
    "            chroma_segment_cache_policy=\"LRU\",\n",
    "            chroma_memory_limit_bytes=cache_size\n",
    "        )\n",
    "        # Initialize persistent client for Chroma DB\n",
    "        self.client = chromadb.PersistentClient(path=db_path, settings=self.settings)\n",
    "        # Create or get the collection, specifying cosine similarity\n",
    "        self.collection = self.client.get_or_create_collection(collection_name, metadata={\"hnsw:space\": \"cosine\"})\n",
    "        if embedding_model is None:\n",
    "            raise ValueError(\"An embedding_model must be provided.\")\n",
    "        self.embedding_model = embedding_model\n",
    "        self.id_counter = 0  # To assign unique IDs if needed\n",
    "\n",
    "    def embed_csv(self, csv_file_path: str, batch_size: int = 100):\n",
    "        \"\"\"\n",
    "        Reads a CSV file and embeds its content into the collection in batches.\n",
    "        Each row is converted to a JSON string (excluding the 'id' column if present).\n",
    "        If a row's text is too long, it is split into chunks.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(csv_file_path):\n",
    "            raise FileNotFoundError(f\"CSV file not found: {csv_file_path}\")\n",
    "\n",
    "        # Read the CSV into a DataFrame\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "        # Ensure there is an 'id' column; if not, create one\n",
    "        if 'id' not in df.columns:\n",
    "            df['id'] = df.index.astype(str)\n",
    "        \n",
    "        # Convert each row into a dictionary\n",
    "        rows = df.to_dict(orient='records')\n",
    "        \n",
    "        batch_ids = []\n",
    "        batch_documents = []\n",
    "        batch_metadatas = []\n",
    "        \n",
    "        for row in tqdm(rows, desc=\"Embedding CSV rows\"):\n",
    "            # Get the document id (as a string)\n",
    "            doc_id = str(row.get('id', self.id_counter))\n",
    "            # Remove the 'id' field for the embedding\n",
    "            row_copy = {k: v for k, v in row.items() if k != 'id'}\n",
    "            # Convert the remaining data to a JSON string\n",
    "            doc_text = json.dumps(row_copy)\n",
    "            \n",
    "            # Check if the document is too long; if so, split into chunks.\n",
    "            if len(doc_text) > 8000:\n",
    "                chunks = split_text(doc_text, max_chunk_length=8000, overlap_ratio=0.1)\n",
    "                for chunk in chunks:\n",
    "                    batch_documents.append(chunk)\n",
    "                    # Create a unique id for each chunk\n",
    "                    batch_ids.append(f\"{doc_id}_{self.id_counter}\")\n",
    "                    batch_metadatas.append({\"doc_name\": os.path.basename(csv_file_path)})\n",
    "                    self.id_counter += 1\n",
    "            else:\n",
    "                batch_documents.append(doc_text)\n",
    "                batch_ids.append(doc_id)\n",
    "                batch_metadatas.append({\"doc_name\": os.path.basename(csv_file_path)})\n",
    "                self.id_counter += 1\n",
    "\n",
    "            # If the batch is full, upsert into the collection in one call.\n",
    "            if len(batch_documents) >= batch_size:\n",
    "                # Compute embeddings for the entire batch at once.\n",
    "                # Directly convert each embedding (a NumPy array) to a list.\n",
    "                embeddings = [self.embedding_model.encode(doc).tolist() for doc in batch_documents]\n",
    "                self.collection.add(\n",
    "                    documents=batch_documents,\n",
    "                    ids=batch_ids,\n",
    "                    embeddings=embeddings,\n",
    "                    metadatas=batch_metadatas\n",
    "                )\n",
    "                batch_ids = []\n",
    "                batch_documents = []\n",
    "                batch_metadatas = []\n",
    "\n",
    "        # Upsert any remaining documents not in a full batch.\n",
    "        if batch_documents:\n",
    "            embeddings = [self.embedding_model.encode(doc).tolist() for doc in batch_documents]\n",
    "            self.collection.add(\n",
    "                documents=batch_documents,\n",
    "                ids=batch_ids,\n",
    "                embeddings=embeddings,\n",
    "                metadatas=batch_metadatas\n",
    "            )\n",
    "        \n",
    "        print(f\"Finished embedding CSV: {csv_file_path}\")\n",
    "\n",
    "    def query_collection(self, query: str, n_results: int = 5) -> dict:\n",
    "        \"\"\"\n",
    "        Queries the collection using the provided query string and returns the results.\n",
    "        \"\"\"\n",
    "        query_embedding = self.embedding_model.encode(query).tolist()\n",
    "        results = self.collection.query(query_embeddings=query_embedding, n_results=n_results, include=['documents', 'metadatas', 'distances'])\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding CSV rows:   2%|▏         | 99/6516 [00:00<00:03, 1938.33it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.float32' object has no attribute 'embedding'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 9\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# create an instance of the csv embedding manager\u001b[39;00m\n\u001b[1;32m      7\u001b[0m memory_manager \u001b[38;5;241m=\u001b[39m ImprovedCSVEmbeddingManager(collection_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_ml_memory\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m                                              embedding_model\u001b[38;5;241m=\u001b[39membedding_model)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mmemory_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/renttherunway_cleaned.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Define sample inputs for the pipeline.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m preprocessing_input \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI have uploaded the dataset obtained from Rent the Runway, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhich relates to fit fiber clothing for women. Develop a model with at least 90 percent F1 score. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe target variable is fit.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     17\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[21], line 94\u001b[0m, in \u001b[0;36mImprovedCSVEmbeddingManager.embed_csv\u001b[0;34m(self, csv_file_path, batch_size)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# If the batch is full, upsert into the collection in one call.\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(batch_documents) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size:\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;66;03m# Compute embeddings for the entire batch at once.\u001b[39;00m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;66;03m# (Assuming your embedding model supports batch processing.)\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_model\u001b[38;5;241m.\u001b[39mencode(doc)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m batch_documents]\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollection\u001b[38;5;241m.\u001b[39madd(\n\u001b[1;32m     96\u001b[0m         documents\u001b[38;5;241m=\u001b[39mbatch_documents,\n\u001b[1;32m     97\u001b[0m         ids\u001b[38;5;241m=\u001b[39mbatch_ids,\n\u001b[1;32m     98\u001b[0m         embeddings\u001b[38;5;241m=\u001b[39membeddings,\n\u001b[1;32m     99\u001b[0m         metadatas\u001b[38;5;241m=\u001b[39mbatch_metadatas\n\u001b[1;32m    100\u001b[0m     )\n\u001b[1;32m    101\u001b[0m     batch_ids \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[21], line 94\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# If the batch is full, upsert into the collection in one call.\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(batch_documents) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size:\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;66;03m# Compute embeddings for the entire batch at once.\u001b[39;00m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;66;03m# (Assuming your embedding model supports batch processing.)\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m batch_documents]\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollection\u001b[38;5;241m.\u001b[39madd(\n\u001b[1;32m     96\u001b[0m         documents\u001b[38;5;241m=\u001b[39mbatch_documents,\n\u001b[1;32m     97\u001b[0m         ids\u001b[38;5;241m=\u001b[39mbatch_ids,\n\u001b[1;32m     98\u001b[0m         embeddings\u001b[38;5;241m=\u001b[39membeddings,\n\u001b[1;32m     99\u001b[0m         metadatas\u001b[38;5;241m=\u001b[39mbatch_metadatas\n\u001b[1;32m    100\u001b[0m     )\n\u001b[1;32m    101\u001b[0m     batch_ids \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.float32' object has no attribute 'embedding'"
     ]
    }
   ],
   "source": [
    "state = State(phase=\"Model development\", competition=\"MyCompetition\")\n",
    "state.make_context() # build context info\n",
    "\n",
    "# embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# create an instance of the csv embedding manager\n",
    "memory_manager = ImprovedCSVEmbeddingManager(collection_name=\"auto_ml_memory\",\n",
    "                                             embedding_model=embedding_model)\n",
    "memory_manager.embed_csv(\"data/renttherunway_cleaned.csv\")\n",
    "\n",
    "\n",
    "# Define sample inputs for the pipeline.\n",
    "preprocessing_input = (\n",
    "        \"I have uploaded the dataset obtained from Rent the Runway, \"\n",
    "        \"which relates to fit fiber clothing for women. Develop a model with at least 90 percent F1 score. \"\n",
    "        \"The target variable is fit.\"\n",
    "    )\n",
    "model_request = \"Find the top 3 models for classifying this dataset.\"\n",
    "deployment_details = \"Deploy the selected model as a web application.\"\n",
    "\n",
    "# Create the pipeline agent with the dictionary of agents and dataset directory.\n",
    "pipeline = PipelineAgent(agents=agents, state=state, memory_manager=memory_manager,\n",
    "                         dataset_dir=\"data\")\n",
    "\n",
    "# Execute the pipeline\n",
    "results = pipeline.run_pipeline(preprocessing_input, model_request, deployment_details)\n",
    "\n",
    "# Optionally, print the results for debugging.\n",
    "print(\"Pipeline execution completed. Results:\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multi-agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
